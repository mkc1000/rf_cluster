import numpy as np
from rfcluster import RFCluster, JKMeans
from within_cluster_variance import WCVScore
from sklearn.cluster import DBSCAN
from sklearn.pipeline import Pipeline
from sklearn.datasets import load_boston

def jaccard(x,y):
    return np.mean(x!=y)

boston = load_boston()
data = boston.data

# dbscan = DBSCAN(13, 7)
# rfc_dbscan = Pipeline([('rfc', RFCluster(50,3,0.2,5)), ('jkmeans', JKMeans(5, n_attempts=30))])
#
# print rfc_dbscan.fit_predict(data)
#
# wcv_dbscan = WCVScore(dbscan)
# wcv_rfc_dbscan = WCVScore(rfc_dbscan, sample=0.5)
#
# print wcv_dbscan.score(data)
# print wcv_rfc_dbscan.score(data)

n_trees = [5,6,7,8]
n_features_to_predict = [0.1,0.3,0.5]
max_depth = [4,5,6]

log = [(1, 0.1, 1, 0.9953352973025015), (1, 0.1, 2, 0.8000489926795535), (1, 0.1, 3, 0.8346657290450159), (1, 0.1, 4, 0.7741172149067584), (1, 0.1, 5, 0.6989880555518303), (1, 0.1, 6, 0.8413774530896604), (1, 0.3, 1, 0.960253895648647), (1, 0.3, 2, 0.7262577074219349), (1, 0.3, 3, 0.6767744977190913), (1, 0.3, 4, 0.6926170817143567), (1, 0.3, 5, 0.6334677042790839), (1, 0.3, 6, 0.7232331641525119), (1, 0.5, 1, 0.9986310622461143), (1, 0.5, 2, 0.8813884661507674), (1, 0.5, 3, 0.6528480534630685), (1, 0.5, 4, 0.7260809524296227), (1, 0.5, 5, 0.6113146384858719), (1, 0.5, 6, 0.71329510964770049), (2, 0.1, 1, 0.95934478307161486), (2, 0.1, 2, 0.94098191964831446), (2, 0.1, 3, 0.76682899242640257), (2, 0.1, 4, 0.84119339174747043), (2, 0.1, 5, 0.80486189027960375), (2, 0.1, 6, 0.94595572912684089), (2, 0.3, 1, 0.99631071999820364), (2, 0.3, 2, 0.8203039356778965), (2, 0.3, 3, 0.70915295839202996), (2, 0.3, 4, 0.64607309101693411), (2, 0.3, 5, 0.71142852962528935), (2, 0.3, 6, 0.70967814234798798), (2, 0.5, 1, 0.93854112849239157), (2, 0.5, 2, 0.74168796493795541), (2, 0.5, 3, 0.84891809966896215), (2, 0.5, 4, 0.77701410769632795), (2, 0.5, 5, 0.67681399964533451), (2, 0.5, 6, 0.65096846004450459), (3, 0.1, 1, 0.84392355752118131), (3, 0.1, 2, 0.62640202956230584), (3, 0.1, 3, 0.92447047244316505), (3, 0.1, 4, 0.6881334266959408), (3, 0.1, 5, 0.82059672589760524), (3, 0.1, 6, 0.80871952497973709), (3, 0.3, 1, 0.97149339562051429), (3, 0.3, 2, 0.83798950017334373), (3, 0.3, 3, 0.72840858110316031), (3, 0.3, 4, 0.66195963776431388), (3, 0.3, 5, 0.7685662938541783), (3, 0.3, 6, 0.77712110841340543), (3, 0.5, 1, 0.96184340508129418), (3, 0.5, 2, 0.86865311607256701), (3, 0.5, 3, 0.87115776341390427), (3, 0.5, 4, 0.754664190098985), (3, 0.5, 5, 0.71421215922248038), (3, 0.5, 6, 0.77123634402802854), (4, 0.1, 1, 0.74033826318675378), (4, 0.1, 2, 0.9417718463035949), (4, 0.1, 3, 0.73084518376338525), (4, 0.1, 4, 0.85892054371251991), (4, 0.1, 5, 0.77724358269103522), (4, 0.1, 6, 0.80896498282159135), (4, 0.3, 1, 0.97168007966741021), (4, 0.3, 2, 0.73186085378540766), (4, 0.3, 3, 0.76091094968225292), (4, 0.3, 4, 0.71240813820194981), (4, 0.3, 5, 0.80338172919868023), (4, 0.3, 6, 0.80963665268157914), (4, 0.5, 1, 0.95981502509479111), (4, 0.5, 2, 0.90474835115301111), (4, 0.5, 3, 0.96719922641955058), (4, 0.5, 4, 0.72613073903398151), (4, 0.5, 5, 0.75739857247699738), (4, 0.5, 6, 0.76331614747405874), (5, 0.1, 1, 0.94228231641010407), (5, 0.1, 2, 0.84372187714737457), (5, 0.1, 3, 0.73200504279606138), (5, 0.3, 1, 0.89968455026044503), (5, 0.3, 2, 0.9109645967494141), (5, 0.3, 3, 0.73320584531828914), (5, 0.5, 1, 0.97730369448641008), (5, 0.5, 2, 0.71630001509190255), (5, 0.5, 3, 0.84307345799660771), (6, 0.1, 1, 0.92104867762375708), (6, 0.1, 2, 0.92417897803444826), (6, 0.1, 3, 0.72920183919387216), (6, 0.3, 1, 0.9449212480065815), (6, 0.3, 2, 0.87107052769117999), (6, 0.3, 3, 0.73687006980244929), (6, 0.5, 1, 0.9586898852271456), (6, 0.5, 2, 0.93399028355559988), (6, 0.5, 3, 0.87552844721937317), (7, 0.1, 1, 0.84366264164156246), (7, 0.1, 2, 0.94807506526191032), (7, 0.1, 3, 0.82076890953699611), (7, 0.3, 1, 0.90706149436906647), (7, 0.3, 2, 0.82848620524535244), (7, 0.3, 3, 0.6882838065472503), (7, 0.5, 1, 0.95968702382937643), (7, 0.5, 2, 0.93340384648384622), (7, 0.5, 3, 0.86730690397050769), (8, 0.1, 1, 0.94744078620452443), (8, 0.1, 2, 0.9737976520171634), (8, 0.1, 3, 0.879812288757431), (8, 0.3, 1, 0.93174997019488504), (8, 0.3, 2, 0.84348737298230847), (8, 0.3, 3, 0.70671950291486685), (8, 0.5, 1, 0.95211608069716125), (8, 0.5, 2, 0.87812207424336031), (8, 0.5, 3, 0.72567479615114949)]

# with open('log.csv', 'a') as f:
#     for tup in log:
#         f.write(','.join([str(i) for i in tup]))
#         f.write('\n')
with open('log.csv', 'a') as f:
    for n_tree in n_trees:
        for n_feature_to_predict in n_features_to_predict:
            for max_dep in max_depth:
                rfc = Pipeline([('rfc', RFCluster(60,n_tree,n_feature_to_predict,max_dep)), ('jkmeans', JKMeans(5, n_attempts=6))])
                wcv = WCVScore(rfc)
                score = wcv.score(data)
                tup = (n_tree,n_feature_to_predict,max_dep,score)
                log.append(tup)
                print tup

                f.write(','.join([str(i) for i in tup]))
                f.write('\n')


# epss = range(15,50,2)
# min_sampless = range(2,15,2)
#
# log = []
# for eps in epss:
#     for min_samples in min_sampless:
#         dbscan = DBSCAN(eps,min_samples)
#         n_categories = np.max(dbscan.fit_predict(data)) + 2
#         wcv = WCVScore(dbscan)
#         score = wcv.score(data)
#         log.append((eps, min_samples, n_categories, score))
#         print "eps: ", eps, " min_samples: ", min_samples, " n_categories: ", n_categories
#         print "score: ", score
#
# log = np.array(log)
# minimizers = []
# category_counts = np.unique(log[:,2])
# for category_count in category_counts:
#     sub_log = log[log[:,2]==category_count]
#     minimizers.append(list(sub_log[np.argmin(sub_log[:,3])]))
# minimizers = np.array(minimizers)
